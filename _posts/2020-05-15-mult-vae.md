---
keywords: fastai
description: An deep dive into the use of Variational Autoencoders for Collaborative Filtering
title: "RecoTour III: Variational Autoencoders for Collaborative Filtering with Mxnet and Pytorch"
author: Javier Rodriguez
toc: true 
badges: true
comments: true
categories: [jupyter]
nb_path: _notebooks/2020-05-15-mult-vae.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-05-15-mult-vae.ipynb
-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This post and the code here are part of a larger repo called <a href="https://github.com/jrzaurin/RecoTour">RecoTour</a>, where I normally explore and implement some recommendation algorithms that I consider interesting and/or useful (see <a href="https://medium.com/datadriveninvestor/recotour-a-tour-through-recommendation-algorithms-in-python-52d780628ab9">RecoTour</a> and <a href="https://towardsdatascience.com/recotour-ii-neural-recommendation-algorithms-49733938d56e">RecoTourII</a>). In every directory, I have included a <code>README</code> file and a series of explanatory notebooks that I hope help explaining the code. I keep adding algorithms from time to time, so stay tunned if you are interested.</p>
<p>As always, let me first acknowledge the relevant people that did the hard work. This post and the companion repo are based on the paper “Variational Autoencoders for Collaborative Filtering” [1]. The code in that repo is partially inspired in the implementation by <a href="https://github.com/younggyoseo/vae-cf-pytorch">Younggyo Seo</a>. I have adapted the code to my coding preferences and added some options and flexibility to run multiple experiment.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="1.-Partially-Regularized-Multinomial-Variational-&#160;Autoencoder">1. Partially Regularized Multinomial Variational &#160;Autoencoder<a class="anchor-link" href="#1.-Partially-Regularized-Multinomial-Variational-&#160;Autoencoder"> </a></h2><p>...or $\text{Mult-VAE}^{\text{PR}}$...I must admit that when it comes to variational autoencoders (VAEs) I find that there is a "notable" difference between the complexity of the math and that of the code (or maybe is just me that I am not a mathematician). Nonetheless, I think that speaking about VAEs and not mentioning log likelihoods, Evidence Lower Bound (EBLO) or the Kullback–Leibler divergence ($\text{D}_{\text{KL}}$) is almost like "cheating". With that in mind I will try to give some mathematical context to the $\text{Mult-VAE}^{\text{PR}}$ for collaborative filtering and then move to the code, from how one prepares the data to a discussion of the results. Bear in mind that the whole purpose of the math below is to justify the loss function we will be using when training the $\text{Mult-VAE}^{\text{PR}}$ as well as the architecture of the algorithm.</p>
<p>Before diving into the problem scenario and the mathematical formulation, let me describe the notational convention. Following <a href="https://arxiv.org/pdf/1802.05814.pdf">Liang et al., 2018</a>, I will use $u \in \{1,\dots,U\}$ to index users and $i \in \{1,\dots,I\}$ to index items. The user-by-item <strong>binary</strong> interaction matrix (i.e. the click matrix) is $\mathbf{X} \in \mathbb{N}^{U\times I}$ and I will use lower case $\mathbf{x}_u =[X_{u1},\dots,X_{uI}]^\top \in \mathbb{N}^I$ to refer to the click history of an individual user $u$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="1.1-Problem-scenario">1.1 Problem scenario<a class="anchor-link" href="#1.1-Problem-scenario"> </a></h3><p>We are given a dataset $\mathbf{X} = \{ {\mathbf{x}_u} \}^{U}_{u=1}$ of user clicks (a more general scenario is described in "<a href="https://arxiv.org/pdf/1312.6114.pdf">Auto-Encoding Variational Bayes</a>" [2]). Our job is to estimate the parameters of the underlying probability distribution so that we can do inference. In other words, we need to find a statistical model of the data. To do this, we need to maximize the likelihood function so that under the assumed statistical model the observed data is most probable.</p>
<p>To find the maximum likelihood we normally assume that the statistical model of the data involves some latent variable $\bf{z}$, so that the marginal likelihood can be written as:</p>
$$
p_{\theta}(\mathbf{x}_u) = \int {p_{\theta}(\mathbf{z}_u)p_{\theta}(\mathbf{x}_u \vert \mathbf{z}_u) d\mathbf{z}_u} \hspace{1cm} (1) 
$$<p>Eq (1) is solvable if we assume that both the prior $p_{\theta}(\mathbf{z}_u)$ and the conditional probability $p_{\theta}(\mathbf{x}_u \vert \mathbf{z}_u)$ come from parametric families of distributions and that their PDFs are differentiable almost everywhere w.r.t. both $\theta$ and $\mathbf{z}_u$. However, for "<em>moderately</em>" complicated likelihood functions $p_{\theta}(\mathbf{x}_u \vert \mathbf{z}_u)$, such as a neural network with a nonlinear layer, Eq (1) is not intractable (it is not possible to evaluate of differentiate the marginal likelihood). Furthermore, the true posterior  $p_{\theta}(\mathbf{z}_u \vert \mathbf{x}_u) = p_{\theta}(\mathbf{x}_u \vert \mathbf{z}_u)p_{\theta}(\mathbf{z}_u)/p_{\theta}(\mathbf{x}_u)$ is also intractable, and therefore we cannot use an EM algorithm (since the E-step involves the computation of the true posterior at a given iteration).</p>
<p>To address these, and some other limitations, <a href="https://arxiv.org/pdf/1312.6114.pdf">Kingma and Welling 2014</a> proposed a flexible neural network based approach.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="1.2-Auto-Encoding-Variational-Bayes">1.2 Auto-Encoding Variational Bayes<a class="anchor-link" href="#1.2-Auto-Encoding-Variational-Bayes"> </a></h3><p>The following Section is both a summary and my understanding of the paper "<a href="https://arxiv.org/pdf/1312.6114.pdf">Auto-Encoding Variational Bayes</a>" to which I keep referring and that I strongly recommend reading.</p>
<p>Let me remind you: our goal is to maximize the likelihood, or more conveniently the log likelihood $\log p_{\theta}(\mathbf{X})$, where:</p>
$$
\log p_{\theta}(\mathbf{X}) = \sum_u \log p_{\theta}(\mathbf{x}_u) \hspace{1cm} (2) 
$$<p>Each term in the summation can be re-written as:</p>
$$
\log p_{\theta}(\mathbf{x}_u) = D_{KL}\left(q_\phi(\textbf{z}_u\vert \textbf{x}_u) \| p_\theta(\textbf{z}_u \vert \textbf{x}_u)\right) + \underbrace{\mathbb{E} \small{ q_{\phi}(\mathbf{z}_u \vert \mathbf{x}_u) } \left[ -\log q_{\phi}(\mathbf{z}_u \vert \mathbf{x_u}) + \log p_{\theta}(\mathbf{x}_u, \mathbf{z}_u) \right]}_{ELBO \mathcal L(\textbf{x}_u, \phi,\theta)} \hspace{1cm} (3)
$$<p>Where the first elements in the right hand side is the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback–Leibler divergence</a> ($\text{D}_{\text{KL}}$) and $q_\phi(\textbf{z}_u\vert \textbf{x}_u)$ is the approximate posterior of the true posterior $p_\theta(\textbf{z}_u \vert \textbf{x}_u)$. Eq (3) is our "<em>point of entry</em>" from which we will derive the remaining equations. If you want proof of Eq (3) I would recommend reading <a href="https://vannevar.ece.uw.edu/techsite/papers/documents/UWEETR-2010-0002.pdf">this tutorial</a> or this <a href="https://medium.com/@jonathan_hui/machine-learning-summary-proof-terms-8ca7c588905e">"crazy" post</a>.</p>
<p>Moving on, given that $\text{D}_{\text{KL}}$ is non-negative, $\log p_{\theta}(\mathbf{x}_u) \geq  \mathcal L(\textbf{x}_u, \phi,\theta)$ and therefore  $\mathcal L$ is referred as Evidence Lower Bound (ELBO). It is straightforward to understand from Eq (3) that maximizing $\log p_{\theta}(\mathbf{x}_u)$ implies maximizing ELBO $\mathcal L$. If we re-order the terms in that equation, we could also think of the problem as follows: maximizing ELBO $\mathcal L$ implies minimizing $\text{D}_{\text{KL}}$, which makes sense, since $D_{KL}$ measures the dissimilarity between the approximate posterior  $q_\phi(\textbf{z}_u\vert \textbf{x}_u)$ and the true posterior $p_{\theta}(\textbf{z}_u\vert \textbf{x}_u)$.</p>
<p>ELBO $\mathcal L$ in Eq (3) can also be re-written as:</p>
$$
\mathcal L(\textbf{x}_u, \phi,\theta) =  - D_{KL}\left(q_\phi(\textbf{z}_u\vert \textbf{x}_u) \| p_\theta(\textbf{z}_u \right) + \mathbb{E} \small{ q_{\phi}(\mathbf{z}_u \vert \mathbf{x}_u) } \left[ \log p_{\theta}(\textbf{x}_u\vert \textbf{z}_u) \right] \hspace{1cm} (4)
$$<p>We can see that Eq (4) involves sampling $\tilde{\mathbf{z}_u} \sim q_{\phi}(\mathbf{z}_u \vert \mathbf{x}_u)$. When sampling is involved, backpropagation is not trivial (how one would take gradients with respect to $\phi$?). To remedy this situation the authors used the so called "<em>reparameterization trick</em>". Instead of sampling from the approximate postertior $q_{\phi}(\mathbf{z}_u \vert \mathbf{x}_u)$, the authors used a differentiable transformation $g_{\phi}(\mathbf{\epsilon}, \mathbf{x}_u)$ of a noise variable $\epsilon$, such that:</p>
$$
\tilde{\mathbf{z}_u} = g_{\phi}(\mathbf{\epsilon}, \mathbf{x}_u) \hspace{1cm} with \hspace{1cm} \mathbf{\epsilon} \sim p(\epsilon) \hspace{1cm} (5)
$$<p>where $p(\epsilon)$ can be, for example, a variable sampled from a random normal distribution (see Section 1.3 for the selection of $g_{\phi}$ in the particular case of the $\text{Mult-VAE}^{\text{PR}}$). With these formulation, one can use Monte Carlo estimates of expectations of some function $f(\mathbf{z})$ with respect to $q_{\phi}(\mathbf{z}_u \vert \mathbf{x}_u)$ such that:</p>
$$
\mathbb{E} \small{ q_{\phi}(\mathbf{z}_u \vert \mathbf{x}_u) }\left[ f(\mathbf{z}_u) \right] =  \mathbb{E} \small{ q_{\phi}(\mathbf{z}_u \vert \mathbf{x}_u) }\left[ f(g_{\phi}(\mathbf{\epsilon}, \mathbf{x}_u)) \right] \simeq \frac{1}{L} \sum_{l=1}^{L} f(g_{\phi}(\mathbf{\epsilon}^l), \mathbf{x}_u) 
\\
\text{where} \hspace{1cm} \mathbf{\epsilon}^l \sim p(\epsilon) \hspace{1cm} (6)
$$<p>Replacing the second term in Eq (4) with the result in Eq (6), we see that the ELBO $\mathcal L$ can be approximated by what Kingma and Welling called generic Stochastic Gradient Variational Bayes (SGVB) estimator $\tilde{\mathcal L}(\textbf{x}_u, \phi,\theta) \simeq \mathcal L(\textbf{x}_u, \phi,\theta)$:</p>
$$
\tilde{\mathcal L}(\mathbf{x}_u, \phi,\theta) = - D_{KL}\left(q_\phi(\textbf{z}_u\vert \textbf{x}_u) \| p_\theta(\textbf{z}_u \right) + \frac{1}{L} \sum_{l=1}^{L} \log p_{\theta}(\mathbf{x}_u \vert \mathbf{z}^l_u) \\
\text{where} \hspace{1cm} \mathbf{z}^l_u = g_{\phi}(\epsilon^l_u,  \mathbf{x}_u) \hspace{1cm} \text{and} \hspace{1cm} \epsilon^l \sim p(\epsilon) \hspace{1cm} (7)
$$<p>Before we move on to Multinomial variational autoencoders, there is one more important result to discuss. When running a practical application, we will be using minibatches, so that:</p>
$$
\mathcal L(\mathbf{\text{X}}^M, \phi,\theta) \simeq \tilde{\mathcal L}^{M}(\mathbf{\text{X}}^M, \phi,\theta) = \frac{1}{M} \sum_{u=1}^{M} \tilde{\mathcal L}(\mathbf{x}_u, \phi,\theta) \hspace{1cm} (8)
$$<p>where $\mathbf{X}^M = \{\mathbf{x}_u \}_{u=1}^M$ is a minibatch of M users. In their experiments the authors found that the number of samples $L$ can be set to 1 as long as the minibatch size was large enough, e.g. $M$ = 100. With this in mind, as long as our batch sizes are of 100 or more, Eq (7) can be re-written as:</p>
$$
\mathcal L(\mathbf{\text{X}}^M, \phi,\theta) \simeq \frac{1}{M} \sum_{u=1}^{M} - D_{KL}\left(q_\phi(\textbf{z}_u\vert \textbf{x}_u) \| p_\theta(\textbf{z}_u \right) + \log  p_{\theta}(\mathbf{x}_u \vert \mathbf{z}^s_u) \hspace{1cm} (9)
$$<p>Note that $\mathbf{z}^s_u$ signifies that $\mathbf{z}_u$ stil needs to be sampled once from $q_\phi(\textbf{z}_u\vert \textbf{x}_u)$, but using the reparameterization trick this will be rather easy, as we will see in the next section. Finally, now that we have a "nice looking" mathematical expression, this is how Auto-Encoding Variational Bayes works:</p>
<ol>
<li>Select a prior for latent representation of $\textbf{x}_u$, $p_{\theta}(\textbf{z}_u)$</li>
<li>Use a neural network to parameterize the distribution $p_{\theta}(\textbf{x}_u\vert \textbf{z}_u)$. Because this part of the model maps the latent variable/representation $\textbf{z}_u$ to the observed data $\textbf{x}_u$, it is referred as a "<em>decoder</em>" network. </li>
<li>Rather than explicitly calculating the intractable posterior $p_{\theta}(\textbf{z}_u\vert \textbf{x}_u)$, use another another neural network to parameterize the distribution $q_\phi(\textbf{z}_u\vert \textbf{x}_u)$ as the approximate posterior. Since $q_\phi$ maps the observed data $\textbf{x}_u$ to the latent space of $\textbf{z}_u$'s, is referred as the "<em>encoder</em>" network.</li>
<li>maxmize ELBO $\mathcal{L}$ in Eq (9) using Stochastic Gradient Descent or any of its cousins</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="1.3-The-$\text{Mult-VAE}^{\text{PR}}$">1.3 The $\text{Mult-VAE}^{\text{PR}}$<a class="anchor-link" href="#1.3-The-$\text{Mult-VAE}^{\text{PR}}$"> </a></h3><p>The set up used by <a href="https://arxiv.org/pdf/1802.05814.pdf">Liang and co-authors</a> is the following: for each user $u$, the latent representation $\textbf{z}_u$ is assumed to be drawn from a standard Gaussian prior $p(\textbf{z}_u) \sim \mathcal N(0, I)$. Such representation is then transformed by a multi-layer perceptron (MLP), and the output is normalized via a Softmax function to produce a probability distribution over all items <strong>$I$</strong>, $\pi(\mathbf{z}_u) = Softmax(MLP(\mathbf{z}_u))$. Then, the click history of user $u$ is assumed to be drawn from a Multinomial distribution with probability $\pi(\mathbf{z}_u)$:</p>
$$
\textbf{x}_u \sim \text{Mult}(N_u, \pi(\mathbf{z}_u)) \hspace{1cm} (10)
$$<p>where $N_u = \sum_i x_{ui}$ is the total number of clicks for user $u$. In this set up, the log-likelihood of the click history $\mathbf{x}_u$ conditioned to the latent representation $\mathbf{z}_u$ is simply:</p>
$$
\begin{equation*}
\log(p_{\theta}(\textbf{x}_u\vert \textbf{z}_u)) = \mathbf{x}_u \log(\pi(\mathbf{z}_u)) \hspace{1cm} (11)
\end{equation*}
$$<p>The posterior $q_\phi(\textbf{z}_u\vert \textbf{x}_u)$ is also chosen to be a standard Gaussian $q_\phi(\textbf{z}_u\vert \textbf{x}_u) \sim \mathcal N(\mu_\phi(\textbf{x}_u), \sigma_\phi(\textbf{x}_u) I)$ where $\mu_\phi(\textbf{x}_u)$ and  $\sigma_\phi(\textbf{x}_u)$ are functions implemented as neural networks. Then, we use the reparameterization trick and chose $g_{\phi}(\mathbf{\epsilon}, \mathbf{x}_u) = \mu(\textbf{x}_u) + \sigma(\textbf{x}_u) \cdot \epsilon$, where $\epsilon \sim \mathcal{N}(0,I)$. This way $\mathbf{z}^s_u = \mu(\textbf{x}_u) + \sigma(\textbf{x}_u) \cdot \epsilon$ where we sample directly $\epsilon$.</p>
<p>At this stage we have defined the Gaussian prior, the Gaussian approximate posterior and our sampled latent representation. We are finally ready to write the loss function that we will minimize when training the Mult-VAE:</p>
$$
Loss = -\frac{1}{M} \sum_{u=1}^{M} \left[ \mathbf{x}_u \log(\pi(\mathbf{z}_u)) + \frac{\beta}{2}  \sum_j ( 1 + \log(\sigma_{uj}^2) - \mu_{uj}^2 - \sigma_{uj}^2 )  \right] \hspace{1cm} (12)
$$<p>Note that the expression above is the negative ELBO $\mathcal L$ (maximizing $\mathcal L$ is equivalent to minimize -$\mathcal L$) with a multiplicative factor $\beta$ applied to the $D_{KL}$. For the behind the $D_{KL}$ expression given this set up have a look here <a href="https://stats.stackexchange.com/questions/318748/deriving-the-kl-divergence-loss-for-vaes">here</a>.</p>
<p>Let me just comment on that $\beta$. Looking at the loss function in Eq (12) within the context of VAEs, we can see that the first term is the reconstruction loss, while the $D_{KL}$ act as a regularizer. With that in mind, Liang et al add a factor $\beta$ to control the strength of the regularization, and propose $\beta &lt; 1$. Let's paused for one second and think on what this means. First of all, we are no longer optimizing a lower bound for a given log likelihood. In addition, remember that the $D_{KL}$ divergence measures the similarity between the approximate posterior $q_\phi(\textbf{z}_u\vert \textbf{x}_u)$ and the prior $p_\theta(\textbf{z}_u)$. Therefore, by using $\beta &lt; 1$ we are weakening the influence of the prior constrain $q_\phi(\textbf{z}_u\vert \textbf{x}_u) \approx p_\theta(\textbf{z}_u)$ on the loss. This means that we are less able to generalize to novel user clicks from historical data. However, when building recommendation systems we are often not interested in reproducing precisely click histories (i.e. achieving the best loss) but in making good recommendations (i.e. achieving the best ranking metrics). As the authors show in the <a href="https://arxiv.org/pdf/1802.05814.pdf">paper</a> (and we will see here later), the best ranking metrics are obtained when using $\beta &lt; 1$ and in consequence they name the algorithm Partially Regularized Multinomial Autoencoder or $\text{Mult-VAE}^{\text{PR}}$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.-Preparing-the-data">2. Preparing the data<a class="anchor-link" href="#2.-Preparing-the-data"> </a></h2><p>Throughout this exercise I will use two dataset. The <a href="http://jmcauley.ucsd.edu/data/amazon/">Amazon Movies and TV</a> dataset [3] [4] and the <a href="https://grouplens.org/datasets/movielens/20m/">Movilens</a> dataset. The later is mainly use so I can make sure I am obtaining consistent results to those obtained in the paper. As we will see through the notebook, the Amazon dataset is significantly more challenging that the Movielens dataset.</p>
<p>The data preparation is fairly simple, and is identical for both datasets. Therefore, I will focus here only on the Amazon dataset.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse-hide</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pickle</span>

<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Union</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="s1">&#39;/Users/javier/ml_experiments_python/RecoTour/Amazon/mult-vae/&#39;</span><span class="p">))</span>
<span class="n">datapath</span> <span class="o">=</span> <span class="s2">&quot;/Users/javier/ml_experiments_python/RecoTour/Amazon/mult-vae/data&quot;</span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">DATA_DIR</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">datapath</span><span class="p">)</span>
<span class="n">new_colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;item&quot;</span><span class="p">,</span> <span class="s2">&quot;rating&quot;</span><span class="p">,</span> <span class="s2">&quot;timestamp&quot;</span><span class="p">]</span>

<span class="n">inp_path</span> <span class="o">=</span> <span class="n">DATA_DIR</span> <span class="o">/</span> <span class="s2">&quot;amazon-movies&quot;</span>
<span class="n">filename</span> <span class="o">=</span> <span class="s2">&quot;reviews_Movies_and_TV_5.p&quot;</span>
<span class="n">raw_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="n">inp_path</span> <span class="o">/</span> <span class="n">filename</span><span class="p">)</span>
<span class="n">keep_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;reviewerID&quot;</span><span class="p">,</span> <span class="s2">&quot;asin&quot;</span><span class="p">,</span> <span class="s2">&quot;overall&quot;</span><span class="p">,</span> <span class="s2">&quot;unixReviewTime&quot;</span><span class="p">]</span>
<span class="n">raw_data</span> <span class="o">=</span> <span class="n">raw_data</span><span class="p">[</span><span class="n">keep_cols</span><span class="p">]</span>
<span class="n">raw_data</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">new_colnames</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">raw_data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">raw_data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>(1697533, 4)
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>user</th>
      <th>item</th>
      <th>rating</th>
      <th>timestamp</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ADZPIG9QOCDG5</td>
      <td>0005019281</td>
      <td>4</td>
      <td>1203984000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>A35947ZP82G7JH</td>
      <td>0005019281</td>
      <td>3</td>
      <td>1388361600</td>
    </tr>
    <tr>
      <th>2</th>
      <td>A3UORV8A9D5L2E</td>
      <td>0005019281</td>
      <td>3</td>
      <td>1388361600</td>
    </tr>
    <tr>
      <th>3</th>
      <td>A1VKW06X1O2X7V</td>
      <td>0005019281</td>
      <td>5</td>
      <td>1202860800</td>
    </tr>
    <tr>
      <th>4</th>
      <td>A3R27T4HADWFFJ</td>
      <td>0005019281</td>
      <td>4</td>
      <td>1387670400</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="2.1-Filter-triples-(user,-item,-score)">2.1 Filter triples (user, item, score)<a class="anchor-link" href="#2.1-Filter-triples-(user,-item,-score)"> </a></h3><p>The first thing that the we do is to "filter triples" (hereafter refereed as <code>tp</code>) based on the number of times a user interacted with items (min_user_click) or items that where "interacted with" by a user a given number of times (min_item_click).</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse-hide</span>
<span class="k">def</span> <span class="nf">get_count</span><span class="p">(</span><span class="n">tp</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="nb">id</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">Index</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns `tp` groupby+count by `id`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">playcount_groupbyid</span> <span class="o">=</span> <span class="n">tp</span><span class="p">[[</span><span class="nb">id</span><span class="p">]]</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="nb">id</span><span class="p">,</span> <span class="n">as_index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">count</span> <span class="o">=</span> <span class="n">playcount_groupbyid</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">count</span>


<span class="k">def</span> <span class="nf">filter_triplets</span><span class="p">(</span>
    <span class="n">tp</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">min_user_click</span><span class="p">,</span> <span class="n">min_item_click</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">Index</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">Index</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns triplets (`tp`) of user-item-rating for users/items with </span>
<span class="sd">    more than min_user_click/min_item_click counts</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">min_item_click</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">itemcount</span> <span class="o">=</span> <span class="n">get_count</span><span class="p">(</span><span class="n">tp</span><span class="p">,</span> <span class="s2">&quot;item&quot;</span><span class="p">)</span>
        <span class="n">tp</span> <span class="o">=</span> <span class="n">tp</span><span class="p">[</span><span class="n">tp</span><span class="p">[</span><span class="s2">&quot;item&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">itemcount</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="n">itemcount</span> <span class="o">&gt;=</span> <span class="n">min_item_click</span><span class="p">])]</span>

    <span class="k">if</span> <span class="n">min_user_click</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">usercount</span> <span class="o">=</span> <span class="n">get_count</span><span class="p">(</span><span class="n">tp</span><span class="p">,</span> <span class="s2">&quot;user&quot;</span><span class="p">)</span>
        <span class="n">tp</span> <span class="o">=</span> <span class="n">tp</span><span class="p">[</span><span class="n">tp</span><span class="p">[</span><span class="s2">&quot;user&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">usercount</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="n">usercount</span> <span class="o">&gt;=</span> <span class="n">min_user_click</span><span class="p">])]</span>

    <span class="n">usercount</span><span class="p">,</span> <span class="n">itemcount</span> <span class="o">=</span> <span class="n">get_count</span><span class="p">(</span><span class="n">tp</span><span class="p">,</span> <span class="s2">&quot;user&quot;</span><span class="p">),</span> <span class="n">get_count</span><span class="p">(</span><span class="n">tp</span><span class="p">,</span> <span class="s2">&quot;item&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">tp</span><span class="p">,</span> <span class="n">usercount</span><span class="p">,</span> <span class="n">itemcount</span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">filtered_raw_data</span><span class="p">,</span> <span class="n">user_activity</span><span class="p">,</span> <span class="n">item_popularity</span> <span class="o">=</span> <span class="n">filter_triplets</span><span class="p">(</span>
    <span class="n">raw_data</span><span class="p">,</span> <span class="n">min_user_click</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_item_click</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note that, since I am using the "reviews_Movies_and_TV_5" (i.e. the 5-core dataset, where users and items have at least 5 reviews each) filtered_raw_data has no effect on the Amazon dataset. It does however filter some users/items in the case of the Movilens dataset.</p>
<p>Let's now have a look to the sparsity of the dataset:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse-hide</span>
<span class="n">sparsity</span> <span class="o">=</span> <span class="p">(</span>
    <span class="mf">1.0</span>
    <span class="o">*</span> <span class="n">filtered_raw_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="o">/</span> <span class="p">(</span><span class="n">user_activity</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">item_popularity</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;After filtering, there are </span><span class="si">%d</span><span class="s2"> watching events from </span><span class="si">%d</span><span class="s2"> users and </span><span class="si">%d</span><span class="s2"> movies (sparsity: </span><span class="si">%.3f%%</span><span class="s2">)&quot;</span>
    <span class="o">%</span> <span class="p">(</span>
        <span class="n">filtered_raw_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">user_activity</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">item_popularity</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">sparsity</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>After filtering, there are 1697533 watching events from 123960 users and 50052 movies (sparsity: 0.027%)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Comparing these numbers to those of the Movilens dataset (9990682 watching events from 136677 users and 20720 movies: sparsity: 0.353%. see the <a href="https://github.com/dawenl/vae_cf/blob/master/VAE_ML20M_WWW2018.ipynb">notebook</a> corresponding to the original publication, or the <a href="https://arxiv.org/pdf/1802.05814.pdf">original publication</a> itself) one can see that the Amazon dataset is $\sim$13 times more sparse than the Movielens dataset. In consequence, I one would expect that the algorithm finds it more challenging, resulting in lower ranking metrics.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="2.2-Train,-validation-and-test-split">2.2 Train, validation and test split<a class="anchor-link" href="#2.2-Train,-validation-and-test-split"> </a></h3><p>Once the raw data is filtered, we follow the same procedure than that of the original authors to split the users into training, validation and test users.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse-hide</span>
<span class="k">def</span> <span class="nf">split_users</span><span class="p">(</span>
    <span class="n">unique_uid</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">Index</span><span class="p">,</span> <span class="n">test_users_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">pd</span><span class="o">.</span><span class="n">Index</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">Index</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">Index</span><span class="p">]:</span>

    <span class="n">n_users</span> <span class="o">=</span> <span class="n">unique_uid</span><span class="o">.</span><span class="n">size</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">test_users_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">n_heldout_users</span> <span class="o">=</span> <span class="n">test_users_size</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">n_heldout_users</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">test_users_size</span> <span class="o">*</span> <span class="n">n_users</span><span class="p">)</span>

    <span class="n">tr_users</span> <span class="o">=</span> <span class="n">unique_uid</span><span class="p">[:</span> <span class="p">(</span><span class="n">n_users</span> <span class="o">-</span> <span class="n">n_heldout_users</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)]</span>
    <span class="n">vd_users</span> <span class="o">=</span> <span class="n">unique_uid</span><span class="p">[(</span><span class="n">n_users</span> <span class="o">-</span> <span class="n">n_heldout_users</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span> <span class="p">:</span> <span class="p">(</span><span class="n">n_users</span> <span class="o">-</span> <span class="n">n_heldout_users</span><span class="p">)]</span>
    <span class="n">te_users</span> <span class="o">=</span> <span class="n">unique_uid</span><span class="p">[(</span><span class="n">n_users</span> <span class="o">-</span> <span class="n">n_heldout_users</span><span class="p">)</span> <span class="p">:]</span>

    <span class="k">return</span> <span class="n">tr_users</span><span class="p">,</span> <span class="n">vd_users</span><span class="p">,</span> <span class="n">te_users</span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">unique_uid</span> <span class="o">=</span> <span class="n">user_activity</span><span class="o">.</span><span class="n">index</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">98765</span><span class="p">)</span>
<span class="n">idx_perm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">unique_uid</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
<span class="n">unique_uid</span> <span class="o">=</span> <span class="n">unique_uid</span><span class="p">[</span><span class="n">idx_perm</span><span class="p">]</span>
<span class="n">tr_users</span><span class="p">,</span> <span class="n">vd_users</span><span class="p">,</span> <span class="n">te_users</span> <span class="o">=</span> <span class="n">split_users</span><span class="p">(</span>
    <span class="n">unique_uid</span><span class="p">,</span> <span class="n">test_users_size</span><span class="o">=</span><span class="mf">0.1</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tr_users</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">vd_users</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">te_users</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>(99168,) (12396,) (12396,)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And this is how the authors set up the experiment: for validation and test they consider "<em>only</em>" items that have been seen during training</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Select the training observations raw data </span>
<span class="n">tr_obsrv</span> <span class="o">=</span> <span class="n">filtered_raw_data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">filtered_raw_data</span><span class="p">[</span><span class="s2">&quot;user&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">tr_users</span><span class="p">)]</span>
<span class="n">tr_items</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">tr_obsrv</span><span class="p">[</span><span class="s2">&quot;item&quot;</span><span class="p">])</span>

<span class="c1"># Save index dictionaries to &quot;numerise&quot; later one</span>
<span class="n">item2id</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span class="n">sid</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">sid</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tr_items</span><span class="p">))</span>
<span class="n">user2id</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span class="n">pid</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">pid</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">unique_uid</span><span class="p">))</span>

<span class="n">vd_obsrv</span> <span class="o">=</span> <span class="n">filtered_raw_data</span><span class="p">[</span>
    <span class="n">filtered_raw_data</span><span class="p">[</span><span class="s2">&quot;user&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">vd_users</span><span class="p">)</span>
    <span class="o">&amp;</span> <span class="n">filtered_raw_data</span><span class="p">[</span><span class="s2">&quot;item&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">tr_items</span><span class="p">)</span>
<span class="p">]</span>

<span class="n">te_obsrv</span> <span class="o">=</span> <span class="n">filtered_raw_data</span><span class="p">[</span>
    <span class="n">filtered_raw_data</span><span class="p">[</span><span class="s2">&quot;user&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">te_users</span><span class="p">)</span>
    <span class="o">&amp;</span> <span class="n">filtered_raw_data</span><span class="p">[</span><span class="s2">&quot;item&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">tr_items</span><span class="p">)</span>
<span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we have the validation and test users and their interactions, we will split such interactions into so-called "<em>validation and test train and test sets</em>".</p>
<p>I know that this sounds convoluted, but is not that complex. The "<em>validation_train and test_train sets</em>", comprising here 80% of the total validation and test sets, will be used to build what we could think as an input binary <em>"image"</em> to be "<em>encoded -&gt; decoded</em>" by the trained auto-encoder. On the other hand the "<em>validation_test and test_test sets</em>", comprising here 20% of the total validation and test sets, will be used to compute the ranking metrics at validation/test time. If you want more details along with a toy example please go to the corresponding <a href="http://localhost:8790/notebooks/notebooks/01_prepare_data.ipynb">notebook</a> in the repo. I will discuss this further in Section 4.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse-hide</span>
<span class="k">def</span> <span class="nf">split_train_test</span><span class="p">(</span>
    <span class="n">data</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">test_size</span><span class="p">:</span> <span class="nb">float</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">]:</span>

    <span class="n">data_grouped_by_user</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;user&quot;</span><span class="p">)</span>
    <span class="n">tr_list</span><span class="p">,</span> <span class="n">te_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(),</span> <span class="nb">list</span><span class="p">()</span>

    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">98765</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">nm</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data_grouped_by_user</span><span class="p">):</span>
        <span class="n">n_items_u</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">group</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">n_items_u</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">:</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_items_u</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;bool&quot;</span><span class="p">)</span>
            <span class="n">idx</span><span class="p">[</span>
                <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span>
                    <span class="n">n_items_u</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">test_size</span> <span class="o">*</span> <span class="n">n_items_u</span><span class="p">),</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span>
                <span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;int64&quot;</span><span class="p">)</span>
            <span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="n">tr_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">logical_not</span><span class="p">(</span><span class="n">idx</span><span class="p">)])</span>
            <span class="n">te_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tr_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">group</span><span class="p">)</span>

    <span class="n">data_tr</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">tr_list</span><span class="p">)</span>
    <span class="n">data_te</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">te_list</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">data_tr</span><span class="p">,</span> <span class="n">data_te</span>


<span class="k">def</span> <span class="nf">numerize</span><span class="p">(</span><span class="n">tp</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">user2id</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">item2id</span><span class="p">:</span> <span class="n">Dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
    <span class="n">user</span> <span class="o">=</span> <span class="p">[</span><span class="n">user2id</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tp</span><span class="p">[</span><span class="s2">&quot;user&quot;</span><span class="p">]]</span>
    <span class="n">item</span> <span class="o">=</span> <span class="p">[</span><span class="n">item2id</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tp</span><span class="p">[</span><span class="s2">&quot;item&quot;</span><span class="p">]]</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;user&quot;</span><span class="p">:</span> <span class="n">user</span><span class="p">,</span> <span class="s2">&quot;item&quot;</span><span class="p">:</span> <span class="n">item</span><span class="p">},</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;item&quot;</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">vd_items_tr</span><span class="p">,</span> <span class="n">vd_items_te</span> <span class="o">=</span> <span class="n">split_train_test</span><span class="p">(</span><span class="n">vd_obsrv</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">te_items_tr</span><span class="p">,</span> <span class="n">te_items_te</span> <span class="o">=</span> <span class="n">split_train_test</span><span class="p">(</span><span class="n">te_obsrv</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="3-$\text{Mult-VAE}^{\text{PR}}$,-the-code">3 $\text{Mult-VAE}^{\text{PR}}$, the code<a class="anchor-link" href="#3-$\text{Mult-VAE}^{\text{PR}}$,-the-code"> </a></h2><p>After the explanation in Section 1 you might expect the code to look rather complex. However, you might feel disappointed/pleased when you see how simple it really is.</p>
<p>In the <a href="https://arxiv.org/pdf/1802.05814.pdf">original publications</a> the authors used a one hidden layer MLP as generative model. There they say that deeper architectures do not improve the results, which I find it to be true after having run over 120 experiments. With that it mind, let's first have a look the model $ I \rightarrow 600 \rightarrow 200 \rightarrow 600 \rightarrow I$, where $I$ is the total number of items:</p>
<p><img src="/infinitoml/images/copied_from_nb/figures/mvae/multvae_arch.png" alt=""></p>
<p>The colors in the Figure are my attempt to emphasize the <em>reparameterization trick</em>.</p>
<p>In code, the figure above is:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>

<span class="kn">from</span> <span class="nn">mxnet</span> <span class="kn">import</span> <span class="n">autograd</span>
<span class="kn">from</span> <span class="nn">mxnet.gluon</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">HybridBlock</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="3.1-Encoder">3.1 Encoder<a class="anchor-link" href="#3.1-Encoder"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">VAEEncoder</span><span class="p">(</span><span class="n">HybridBlock</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q_dims</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">dropout</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># last dim multiplied by two for the reparameterization trick</span>
        <span class="n">q_dims_</span> <span class="o">=</span> <span class="n">q_dims</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">q_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">]</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">name_scope</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">q_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">HybridSequential</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;q_net&quot;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">out</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">dropout</span><span class="p">,</span> <span class="n">q_dims_</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">q_dims_</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">q_layers</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">q_layers</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">in_units</span><span class="o">=</span><span class="n">inp</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="n">out</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">hybrid_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">L2Normalization</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_layers</span><span class="p">):</span>
            <span class="n">h</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_layers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">h</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_outputs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="3.2-Decoder">3.2 Decoder<a class="anchor-link" href="#3.2-Decoder"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">HybridBlock</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p_dims</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">dropout</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">name_scope</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">p_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">HybridSequential</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;p_net&quot;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">out</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">dropout</span><span class="p">,</span> <span class="n">p_dims</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">p_dims</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">p_layers</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">p_layers</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">in_units</span><span class="o">=</span><span class="n">inp</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="n">out</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">hybrid_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">X</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p_layers</span><span class="p">):</span>
            <span class="n">h</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p_layers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">h</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">h</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="3.3-The-model">3.3 The model<a class="anchor-link" href="#3.3-The-model"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">MultiVAE</span><span class="p">(</span><span class="n">HybridBlock</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">p_dims</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">dropout_enc</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
        <span class="n">dropout_dec</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
        <span class="n">q_dims</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encode</span> <span class="o">=</span> <span class="n">VAEEncoder</span><span class="p">(</span><span class="n">q_dims</span><span class="p">,</span> <span class="n">dropout_enc</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decode</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">p_dims</span><span class="p">,</span> <span class="n">dropout_dec</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">hybrid_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">logvar</span><span class="p">)</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal_like</span><span class="p">(</span><span class="n">std</span><span class="p">)</span>
        <span class="n">sampled_z</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">autograd</span><span class="o">.</span><span class="n">is_training</span><span class="p">()</span> <span class="o">*</span> <span class="n">eps</span> <span class="o">*</span> <span class="n">std</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">sampled_z</span><span class="p">),</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Before I move on, let me mention (and appreciate) one of the many nice "little" things that <code>Mxnet</code>'s <code>Gluon</code> has to offer. You will notice the use of <code>HybridBlock</code> and the use of the input <code>F</code> (the backend) when we define the forward pass, or more precisely, the <code>hybrid_forward</code> pass. One could write a full post on the joys of <code>HybridBlocks</code> and how nicely and easily the guys that developed <code>Gluon</code> brought together the flexibility of imperative frameworks (i.e. <code>Pytorch</code>) and the speed of declarative frameworks (i.e. <code>Tensorflow</code>) together. If you want to learn the details go <a href="https://gluon.mxnet.io/chapter07_distributed-learning/hybridize.html">here</a>, but believe me, this is <strong>FAST</strong>.</p>
<p>Having said that, there is only one more piece that we need to have the complete model, the loss function in Eq (12).</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">vae_loss_fn</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">,</span> <span class="n">anneal</span><span class="p">):</span>
    <span class="c1"># firt term</span>
    <span class="n">neg_ll</span> <span class="o">=</span> <span class="o">-</span><span class="n">nd</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">nd</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">nd</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="o">*</span> <span class="n">inp</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="c1"># second term without beta</span>
    <span class="n">KLD</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">nd</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">nd</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">logvar</span> <span class="o">-</span> <span class="n">nd</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">nd</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logvar</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="c1"># &quot;full&quot; loss</span>
    <span class="k">return</span> <span class="n">neg_ll</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">KLD</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the <a href="https://arxiv.org/pdf/1802.05814.pdf">paper</a> the authors also use a Multinomial Denoising Autoencoder (Mult-DAE). The architecture is identical to that of the $\text{Mult-VAE}^{\text{PR}}$ apart from the fact that there is no variational aspect. I have implemented the Mult-DAE and run multiple experiments with it. However, given its simplicity and an already lengthy post, I will not discuss the corresponding code here.</p>
<p>Let's have a look to the <code>MultiVAE</code></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">I</span> <span class="o">=</span> <span class="mi">50000</span>
<span class="n">q_dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">I</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">600</span><span class="p">,</span> <span class="mi">200</span><span class="p">]</span>
<span class="n">p_dims</span> <span class="o">=</span> <span class="p">[</span><span class="mi">200</span><span class="p">,</span> <span class="mi">600</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">I</span><span class="p">]</span>
<span class="n">dropout_enc</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]</span>
<span class="n">dropout_dec</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">vae_model</span> <span class="o">=</span> <span class="n">MultiVAE</span><span class="p">(</span>
    <span class="n">p_dims</span><span class="o">=</span><span class="n">p_dims</span><span class="p">,</span>
    <span class="n">q_dims</span><span class="o">=</span><span class="n">q_dims</span><span class="p">,</span>
    <span class="n">dropout_enc</span><span class="o">=</span><span class="n">dropout_enc</span><span class="p">,</span>
    <span class="n">dropout_dec</span><span class="o">=</span><span class="n">dropout_dec</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">vae_model</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>MultiVAE(
  (encode): VAEEncoder(
    (q_layers): HybridSequential(
      (0): Dropout(p = 0.5, axes=())
      (1): Dense(50000 -&gt; 600, linear)
      (2): Dropout(p = 0.0, axes=())
      (3): Dense(600 -&gt; 400, linear)
    )
  )
  (decode): Decoder(
    (p_layers): HybridSequential(
      (0): Dropout(p = 0.0, axes=())
      (1): Dense(200 -&gt; 600, linear)
      (2): Dropout(p = 0.0, axes=())
      (3): Dense(600 -&gt; 50000, linear)
    )
  )
)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="4.-Training-the-Model">4. Training the Model<a class="anchor-link" href="#4.-Training-the-Model"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="5.-Results-summary">5. Results summary<a class="anchor-link" href="#5.-Results-summary"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="6.-Future-Work">6. Future Work<a class="anchor-link" href="#6.-Future-Work"> </a></h2>
</div>
</div>
</div>
</div>
 

